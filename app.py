import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# --- å°ˆæ¡ˆæ¨™é¡Œèˆ‡ä»‹ç´¹ ---
st.title("ğŸ· è‘¡è„é…’å“è³ªé æ¸¬ï¼šå¾ç‰¹å¾µé¸æ“‡åˆ°æ¨¡å‹è©•ä¼°")
st.markdown("""
æœ¬æ‡‰ç”¨ç¨‹å¼å°‡å±•ç¤ºä¸€å€‹å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆæµç¨‹ï¼ŒåŒ…æ‹¬è³‡æ–™æº–å‚™ã€ç‰¹å¾µé¸æ“‡ã€æ¨¡å‹å»ºç«‹èˆ‡æ¨¡å‹è©•ä¼°ã€‚
1.  **è³‡æ–™è¼‰å…¥**: å¾ç¶²è·¯è¼‰å…¥ç´…é…’å“è³ªè³‡æ–™é›†ã€‚
2.  **ç¼ºå¤±å€¼è™•ç†**: æª¢æŸ¥ä¸¦è™•ç†è³‡æ–™ä¸­çš„ç¼ºå¤±å€¼ã€‚
3.  **é›¢ç¾¤å€¼åµæ¸¬èˆ‡è™•ç†**: ä½¿ç”¨ IQR æ–¹æ³•é€²è¡Œè™•ç†ã€‚
4.  **ç‰¹å¾µæ¨™æº–åŒ–**: ä½¿ç”¨ Z-score æ¨™æº–åŒ–å°ç‰¹å¾µé€²è¡Œç¸®æ”¾ã€‚
5.  **è³‡æ–™é›†åˆ‡åˆ†**: å°‡è³‡æ–™åˆ‡åˆ†ç‚ºè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ã€‚
6.  **ç‰¹å¾µé¸æ“‡**: ä½¿ç”¨ SelectKBest é¸æ“‡æœ€é‡è¦çš„ç‰¹å¾µã€‚
7.  **æ¨¡å‹è¨“ç·´**: ä½¿ç”¨å¤šå…ƒç·šæ€§å›æ­¸é€²è¡Œè¨“ç·´ã€‚
8.  **æ¨¡å‹è©•ä¼°**: ä½¿ç”¨ MAE, MSE, RMSE, RÂ² è©•ä¼°æ¨¡å‹è¡¨ç¾ã€‚
9.  **è¦–è¦ºåŒ–åˆ†æ**: é€éåœ–è¡¨æ·±å…¥åˆ†ææ¨¡å‹é æ¸¬çµæœã€‚
""")

# --- 1. è³‡æ–™è¼‰å…¥ (Data Loading) ---
st.header("1. è³‡æ–™è¼‰å…¥")
# ä½¿ç”¨åˆ†è™Ÿä½œç‚ºåˆ†éš”ç¬¦è®€å– CSV
try:
    df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')
    st.success("è³‡æ–™è¼‰å…¥æˆåŠŸï¼")
except Exception as e:
    st.error(f"è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ– URLã€‚éŒ¯èª¤è¨Šæ¯ï¼š{e}")
    st.stop()


# --- 2. ç¼ºå¤±å€¼è™•ç† (Missing Value Handling) ---
st.header("2. ç¼ºå¤±å€¼è™•ç†")
if df.isnull().sum().sum() == 0:
    st.info("æ­¤è³‡æ–™é›†éå¸¸ä¹¾æ·¨ï¼Œæ²’æœ‰ç¼ºå¤±å€¼ã€‚")


# --- 3. é›¢ç¾¤å€¼åµæ¸¬èˆ‡è™•ç† (Outlier Detection and Handling) ---
st.header("3. é›¢ç¾¤å€¼åµæ¸¬èˆ‡è™•ç†")
st.markdown("ä½¿ç”¨ IQR æ–¹æ³•å°‡é›¢ç¾¤å€¼èª¿æ•´åˆ°é‚Šç•Œå€¼ã€‚")

feature_cols = df.columns.drop('quality')
df_outlier_handled = df.copy()

for col in feature_cols:
    Q1 = df_outlier_handled[col].quantile(0.25)
    Q3 = df_outlier_handled[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_outlier_handled[col] = np.clip(df_outlier_handled[col], lower_bound, upper_bound)
st.success("é›¢ç¾¤å€¼è™•ç†å®Œæˆï¼")


# --- 4. ç‰¹å¾µæ¨™æº–åŒ– (Feature Standardization) ---
st.header("4. ç‰¹å¾µæ¨™æº–åŒ–")
st.markdown("ä½¿ç”¨ Z-score æ¨™æº–åŒ–ï¼Œå°‡æ‰€æœ‰ç‰¹å¾µè½‰æ›ç‚ºå¹³å‡å€¼ç‚º 0ã€æ¨™æº–å·®ç‚º 1 çš„åˆ†ä½ˆã€‚")

X = df_outlier_handled.drop('quality', axis=1)
y = df_outlier_handled['quality']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)


# --- 5. è³‡æ–™é›†åˆ‡åˆ† (Train/Test Split) ---
st.header("5. è³‡æ–™é›†åˆ‡åˆ†")
st.markdown("å°‡è³‡æ–™é›†ä»¥ 80/20 çš„æ¯”ä¾‹åˆ‡åˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ã€‚")

X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42)


# --- 6. ç‰¹å¾µé¸æ“‡ (Feature Selection) ---
st.header("6. ç‰¹å¾µé¸æ“‡ (Feature Selection)")
st.markdown("""
ä½¿ç”¨ `SelectKBest` æ­é… `f_regression` æª¢å®šä¾†è©•ä¼°æ¯å€‹ç‰¹å¾µå°ç›®æ¨™è®Šæ•¸ (`quality`) çš„é‡è¦æ€§ã€‚
- **F-score**: åˆ†æ•¸è¶Šé«˜ï¼Œä»£è¡¨è©²ç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸çš„ç·šæ€§é—œä¿‚è¶Šå¼·ã€‚
- **p-value**: p-value < 0.05 é€šå¸¸è¡¨ç¤ºè©²ç‰¹å¾µå…·æœ‰çµ±è¨ˆé¡¯è‘—æ€§ã€‚
""")

# è®“ä½¿ç”¨è€…é¸æ“‡è¦ä¿ç•™çš„ç‰¹å¾µæ•¸é‡
k = st.slider("è«‹é¸æ“‡è¦ä¿ç•™çš„ç‰¹å¾µæ•¸é‡ (k)", min_value=1, max_value=X_train.shape[1], value=6, step=1)

# åŸ·è¡Œç‰¹å¾µé¸æ“‡
selector = SelectKBest(score_func=f_regression, k=k)
selector.fit(X_train, y_train)

# é¡¯ç¤ºåˆ†æ•¸
scores = pd.DataFrame({
    'ç‰¹å¾µ': X_train.columns,
    'F-score': selector.scores_,
    'p-value': selector.pvalues_
}).sort_values(by='F-score', ascending=False).reset_index(drop=True)

st.write("å„ç‰¹å¾µçš„é¡¯è‘—æ€§åˆ†æ (F-score & p-value):")
st.dataframe(scores)

# å–å¾—è¢«é¸ä¸­çš„ç‰¹å¾µ
selected_mask = selector.get_support()
selected_features = X_train.columns[selected_mask]
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

st.write(f"ç•¶ k={k} æ™‚ï¼Œè¢«é¸å‡ºçš„é‡è¦ç‰¹å¾µç‚ºï¼š")
st.info(", ".join(selected_features))


# --- 7. å¤šå…ƒç·šæ€§å›æ­¸æ¨¡å‹è¨“ç·´ (Multiple Linear Regression Training) ---
st.header("7. å¤šå…ƒç·šæ€§å›æ­¸æ¨¡å‹è¨“ç·´")
st.markdown("ä½¿ç”¨**è¢«é¸å‡ºçš„ç‰¹å¾µ**ä¾†è¨“ç·´å¤šå…ƒç·šæ€§å›æ­¸æ¨¡å‹ã€‚")

# å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹
model = LinearRegression()
model.fit(X_train_selected, y_train)
st.success("æ¨¡å‹è¨“ç·´å®Œæˆï¼")

# é¡¯ç¤ºæ¨¡å‹ä¿‚æ•¸èˆ‡æˆªè·
st.subheader("æ¨¡å‹ä¿‚æ•¸ (Coefficients) èˆ‡æˆªè· (Intercept)")
coeffs = pd.DataFrame(
    model.coef_,
    X_train_selected.columns,
    columns=['ä¿‚æ•¸ (Coefficient)']
)
st.dataframe(coeffs)
st.write(f"**æ¨¡å‹æˆªè· (Intercept):** `{model.intercept_:.4f}`")


# --- 8. æ¨¡å‹è©•ä¼° (Model Evaluation) ---
st.header("8. æ¨¡å‹è©•ä¼° (Model Evaluation)")
st.markdown("ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹å°**æ¸¬è©¦é›†**é€²è¡Œé æ¸¬ï¼Œä¸¦è¨ˆç®—è©•ä¼°æŒ‡æ¨™ã€‚")

# é€²è¡Œé æ¸¬
y_pred = model.predict(X_test_selected)

# è¨ˆç®—æŒ‡æ¨™
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# é¡¯ç¤ºæŒ‡æ¨™
col1, col2 = st.columns(2)
col3, col4 = st.columns(2)

with col1:
    st.metric("RÂ² (R-squared)", f"{r2:.4f}", help="RÂ² è¡¡é‡æ¨¡å‹å°è³‡æ–™è®Šç•°çš„è§£é‡‹ç¨‹åº¦ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ã€‚RÂ² ç‚ºè² å€¼è¡¨ç¤ºæ¨¡å‹è¡¨ç¾æ¯”ç›´æ¥é æ¸¬å¹³å‡å€¼é‚„å·®ã€‚")
with col2:
    st.metric("MAE (Mean Absolute Error)", f"{mae:.4f}", help="æ‰€æœ‰é æ¸¬èª¤å·®çµ•å°å€¼çš„å¹³å‡ï¼Œå€¼è¶Šå°è¶Šå¥½ã€‚")
with col3:
    st.metric("MSE (Mean Squared Error)", f"{mse:.4f}", help="æ‰€æœ‰é æ¸¬èª¤å·®å¹³æ–¹çš„å¹³å‡ï¼Œå°å¤§èª¤å·®çš„æ‡²ç½°è¼ƒé‡ã€‚")
with col4:
    st.metric("RMSE (Root Mean Squared Error)", f"{rmse:.4f}", help="MSE çš„å¹³æ–¹æ ¹ï¼Œèˆ‡ç›®æ¨™è®Šæ•¸å–®ä½ç›¸åŒï¼Œå€¼è¶Šå°è¶Šå¥½ã€‚")

# --- æ¨¡å‹è¡¨ç¾åˆ†æèˆ‡æ”¹é€²å»ºè­° ---
st.subheader("æ¨¡å‹è¡¨ç¾åˆ†æèˆ‡æ”¹é€²å»ºè­°")
st.markdown(f"""
ç›®å‰çš„ç·šæ€§å›æ­¸æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„ **RÂ² ç´„ç‚º {r2:.2f}**ã€‚

#### æ¨¡å‹å¥½å£åˆ†æï¼š
- RÂ² åˆ†æ•¸è¡¡é‡äº†æ¨¡å‹å¯ä»¥è§£é‡‹ç›®æ¨™è®Šæ•¸ï¼ˆé…’çš„å“è³ªï¼‰è®Šç•°çš„ç™¾åˆ†æ¯”ã€‚é€™å€‹æ•¸å€¼é€šå¸¸ä»‹æ–¼ 0 å’Œ 1 ä¹‹é–“ã€‚
- ç•¶å‰çš„ RÂ² åˆ†æ•¸åä½ï¼Œé€™æ„å‘³è‘—ç›®å‰çš„ç‰¹å¾µå’Œç·šæ€§æ¨¡å‹åªèƒ½è§£é‡‹é…’å“è³ªç´„ **{r2:.0%}** çš„è®Šç•°æ€§ã€‚æ›å¥è©±èªªï¼Œæ¨¡å‹çš„é æ¸¬èƒ½åŠ›ç›¸ç•¶æœ‰é™ï¼Œè¨±å¤šå½±éŸ¿å“è³ªçš„å› ç´ æœªèƒ½è¢«æ¨¡å‹æ•æ‰ã€‚

#### å¯èƒ½çš„æ”¹é€²æ–¹å‘ï¼š
1.  **å˜—è©¦éç·šæ€§æ¨¡å‹**ï¼š
    - é…’çš„å“è³ªèˆ‡åŒ–å­¸æˆåˆ†ä¹‹é–“çš„é—œä¿‚å¯èƒ½ä¸æ˜¯ç°¡å–®çš„ç·šæ€§é—œä¿‚ã€‚å¯ä»¥å˜—è©¦æ›´è¤‡é›œçš„æ¨¡å‹ï¼Œå¦‚ **éš¨æ©Ÿæ£®æ— (Random Forest)** æˆ– **æ¢¯åº¦æå‡æ©Ÿ (Gradient Boosting)**ï¼Œå®ƒå€‘èƒ½æ›´å¥½åœ°æ•æ‰éç·šæ€§ç‰¹å¾µã€‚

2.  **é€²è¡Œç‰¹å¾µå·¥ç¨‹ (Feature Engineering)**ï¼š
    - å‰µé€ æ–°çš„ã€å¯èƒ½æ›´æœ‰é æ¸¬èƒ½åŠ›çš„ç‰¹å¾µã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å˜—è©¦ç”Ÿæˆ `total acidity` (fixed acidity + volatile acidity) æˆ–é…’ç²¾èˆ‡ç¡«é…¸é¹½çš„äº¤äº’ä½œç”¨é … (`alcohol * sulphates`)ã€‚

3.  **å°‡å•é¡Œé‡æ–°å®šç¾©ç‚ºã€Œåˆ†é¡å•é¡Œã€**ï¼š
    - ç”±æ–¼ `quality` æ˜¯ä¸€å€‹ 1 åˆ° 10 çš„æ•´æ•¸è©•åˆ†ï¼Œå¯ä»¥å°‡å…¶è¦–ç‚ºä¸€å€‹åˆ†é¡å•é¡Œè€Œéè¿´æ­¸å•é¡Œã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°‡å“è³ªåˆ†æ•¸ 7 åˆ†ä»¥ä¸Šå®šç¾©ç‚ºã€Œå„ªè³ªé…’ã€ï¼Œ4 åˆ†ä»¥ä¸‹ç‚ºã€ŒåŠ£è³ªé…’ã€ï¼Œå…¶é¤˜ç‚ºã€Œæ™®é€šé…’ã€ï¼Œç„¶å¾Œè¨“ç·´ä¸€å€‹åˆ†é¡æ¨¡å‹ä¾†é æ¸¬é…’çš„ç­‰ç´šã€‚
""")

# --- 9. æ¨¡å‹è¦–è¦ºåŒ–åˆ†æ (Visual Analysis) ---
st.header("9. æ¨¡å‹è¦–è¦ºåŒ–åˆ†æ")

# å»ºç«‹ä¸€å€‹ Figure å’Œå…©å€‹ Axes
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
plt.style.use('seaborn-v0_8-whitegrid')

# --- åœ– 1: é æ¸¬å€¼ vs. å¯¦éš›å€¼ ---
sns.scatterplot(x=y_test, y=y_pred, ax=ax1, alpha=0.6, color='royalblue')
# åŠ ä¸Š 45 åº¦åƒè€ƒç·š (å®Œç¾é æ¸¬ç·š)
max_val = max(y_test.max(), y_pred.max())
min_val = min(y_test.min(), y_pred.min())
ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
ax1.set_xlabel("å¯¦éš›å“è³ª (Actual Quality)")
ax1.set_ylabel("é æ¸¬å“è³ª (Predicted Quality)")
ax1.set_title("é æ¸¬å€¼ vs. å¯¦éš›å€¼æ•£ä½ˆåœ–")
ax1.grid(True)

# --- åœ– 2: æ®˜å·®åœ– ---
residuals = y_test - y_pred
sns.scatterplot(x=y_pred, y=residuals, ax=ax2, alpha=0.6, color='darkorange')
# åŠ ä¸Š 0 ç·š
ax2.axhline(y=0, color='r', linestyle='--', lw=2)
# è¨ˆç®— 95% é æ¸¬å€é–“ç·š
std_dev = np.std(residuals)
upper_bound = 1.96 * std_dev
lower_bound = -1.96 * std_dev
ax2.axhline(y=upper_bound, color='g', linestyle='--', lw=1.5, label='95% é æ¸¬å€é–“')
ax2.axhline(y=lower_bound, color='g', linestyle='--', lw=1.5)
ax2.set_xlabel("é æ¸¬å“è³ª (Predicted Quality)")
ax2.set_ylabel("æ®˜å·® (Residuals = Actual - Predicted)")
ax2.set_title("æ®˜å·®åœ–")
ax2.legend()
ax2.grid(True)


# åœ¨ Streamlit ä¸­é¡¯ç¤ºåœ–è¡¨
st.pyplot(fig)

# --- åœ–è¡¨èªªæ˜ ---
st.subheader("åœ–è¡¨è§£è®€")
st.markdown("""
**1. é æ¸¬å€¼ vs. å¯¦éš›å€¼æ•£ä½ˆåœ– (å·¦åœ–)**
- **ç”¨é€”**: æª¢è¦–æ¨¡å‹é æ¸¬çš„æº–ç¢ºæ€§ã€‚
- **è§£è®€**:
    - è—é»ä»£è¡¨æ¸¬è©¦é›†ä¸­æ¯ä¸€å€‹æ¨£æœ¬çš„ã€Œå¯¦éš›å“è³ªã€èˆ‡ã€Œé æ¸¬å“è³ªã€ã€‚
    - ç´…è‰²è™›ç·šæ˜¯ã€Œå®Œç¾é æ¸¬ç·šã€(y=x)ã€‚å¦‚æœæ‰€æœ‰é»éƒ½è½åœ¨é€™æ¢ç·šä¸Šï¼Œè¡¨ç¤ºæ¨¡å‹é æ¸¬å®Œå…¨æº–ç¢ºã€‚
    - é»è¶Šé è¿‘ç´…è‰²è™›ç·šï¼Œè¡¨ç¤ºé æ¸¬è¶Šæº–ç¢ºã€‚å¾åœ–ä¸­å¯ä»¥çœ‹å‡ºï¼Œé»çš„åˆ†ä½ˆæ¯”è¼ƒç™¼æ•£ï¼Œå†æ¬¡å°è­‰äº† RÂ² åˆ†æ•¸ä¸é«˜çš„çµè«–ã€‚

**2. æ®˜å·®åœ– (å³åœ–)**
- **ç”¨é€”**: è¨ºæ–·æ¨¡å‹çš„ç³»çµ±æ€§èª¤å·®ã€‚ä¸€å€‹å¥½çš„è¿´æ­¸æ¨¡å‹ï¼Œå…¶æ®˜å·®æ‡‰è©²æ˜¯éš¨æ©Ÿåˆ†ä½ˆçš„ã€‚
- **è§£è®€**:
    - æ©«è»¸æ˜¯æ¨¡å‹çš„ã€Œé æ¸¬å€¼ã€ï¼Œç¸±è»¸æ˜¯ã€Œæ®˜å·®ã€ï¼ˆå¯¦éš›å€¼ - é æ¸¬å€¼ï¼‰ã€‚
    - **ç´…è‰²è™›ç·š (y=0)**: ä»£è¡¨æ²’æœ‰èª¤å·®ã€‚é»åœ¨é€™æ¢ç·šä¸Šæ–¹è¡¨ç¤ºæ¨¡å‹ä½ä¼°äº†å¯¦éš›å€¼ï¼›åœ¨ä¸‹æ–¹è¡¨ç¤ºé«˜ä¼°äº†ã€‚
    - **ç†æƒ³æƒ…æ³**: é»æ‡‰è©²åœ¨ y=0 ç·šä¸Šä¸‹éš¨æ©Ÿä¸”å‡å‹»åœ°åˆ†ä½ˆï¼Œæ²’æœ‰ä»»ä½•æ˜é¡¯çš„æ¨¡å¼ï¼ˆå¦‚ U å‹ã€å–‡å­å‹ï¼‰ã€‚
    - **ç¶ è‰²è™›ç·š (95% é æ¸¬å€é–“)**: é€™å…©æ¢ç·šæ¡†å‡ºäº†ç´„ 95% çš„æ®˜å·®æ‰€åœ¨çš„ç¯„åœã€‚å¦‚æœå¤§éƒ¨åˆ†é»éƒ½åœ¨é€™å€‹å€é–“å…§ï¼Œè¡¨ç¤ºæ¨¡å‹çš„é æ¸¬èª¤å·®åœ¨ä¸€å€‹ç›¸å°ç©©å®šçš„ç¯„åœå…§ã€‚è½åœ¨å€é–“å¤–çš„é»å¯è¢«è¦–ç‚ºã€Œé›¢ç¾¤é æ¸¬ã€ï¼Œå€¼å¾—é€²ä¸€æ­¥æ¢è¨ã€‚
""")
